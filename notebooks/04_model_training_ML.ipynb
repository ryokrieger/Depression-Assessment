{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faae56b",
   "metadata": {},
   "source": [
    "# Model Training (Traditional Machine Learning Models)\n",
    "\n",
    "In this notebook, we train and evaluate six classical machine learning models on three different feature sets.\n",
    "\n",
    "**Models used**\n",
    "- Logistic Regression  \n",
    "- Gradient Boosting Classifier  \n",
    "- K-Nearest Neighbour (KNN)  \n",
    "- Random Forest Classifier  \n",
    "- Decision Tree Classifier  \n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "**Feature sets**\n",
    "1Ô∏è‚É£ Feature Set 1 ‚Üí Top 7 features from each PSS-10, GAD-7 and PHQ-9 (21 total)  \n",
    "2Ô∏è‚É£ Feature Set 2 ‚Üí All PSS-10 + All PHQ-9 (19 total)  \n",
    "3Ô∏è‚É£ Feature Set 3 ‚Üí All GAD-7 + All PHQ-9 (17 total)\n",
    "\n",
    "Each model will be evaluated on:\n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F1 Score  \n",
    "and a **Confusion Matrix** for visual assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "FIG_DIR = Path(\"../figures\")\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load feature sets\n",
    "fs1_train = pd.read_csv(DATA_DIR / \"fs1_train.csv\")\n",
    "fs1_test  = pd.read_csv(DATA_DIR / \"fs1_test.csv\")\n",
    "fs2_train = pd.read_csv(DATA_DIR / \"fs2_train.csv\")\n",
    "fs2_test  = pd.read_csv(DATA_DIR / \"fs2_test.csv\")\n",
    "fs3_train = pd.read_csv(DATA_DIR / \"fs3_train.csv\")\n",
    "fs3_test  = pd.read_csv(DATA_DIR / \"fs3_test.csv\")\n",
    "\n",
    "print(\"‚úÖ Feature sets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a685ac2",
   "metadata": {},
   "source": [
    "## Helper Functions for Model Training, Evaluation, and Saving\n",
    "We define functions to:\n",
    "- Train all models,\n",
    "- Evaluate performance (Accuracy, Precision, Recall, F1),\n",
    "- Plot and save confusion matrices,\n",
    "- Save trained model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ce11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, feature_set_name):\n",
    "    \"\"\"Compute metrics, show & save confusion matrix.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_title(f\"Confusion Matrix ‚Äî {model_name} ({feature_set_name})\")\n",
    "\n",
    "    # Save confusion matrix\n",
    "    fig_filename = f\"confusion_{feature_set_name.lower()}_{model_name.lower().replace(' ', '_')}.png\"\n",
    "    fig_path = FIG_DIR / fig_filename\n",
    "    fig.savefig(fig_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"üñºÔ∏è Saved confusion matrix: {fig_path}\")\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_evaluate_all_models(X_train, y_train, X_test, y_test, feature_set_name):\n",
    "    \"\"\"Train six ML models, evaluate, and save each model + confusion matrix.\"\"\"\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîπ Training {name} on {feature_set_name} ...\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_filename = f\"{feature_set_name.lower()}_{name.lower().replace(' ', '_')}.joblib\"\n",
    "        model_path = MODEL_DIR / model_filename\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"üíæ Saved model: {model_path}\")\n",
    "\n",
    "        metrics = evaluate_model(model, X_test, y_test, name, feature_set_name)\n",
    "        results.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7a03a",
   "metadata": {},
   "source": [
    "## Feature Set 1 ‚Äî Top 7 from each scale (21 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = fs1_train.drop(columns=[\"DepressionEncoded\"])\n",
    "y_train1 = fs1_train[\"DepressionEncoded\"]\n",
    "X_test1  = fs1_test.drop(columns=[\"DepressionEncoded\"])\n",
    "y_test1  = fs1_test[\"DepressionEncoded\"]\n",
    "\n",
    "results_fs1 = train_and_evaluate_all_models(X_train1, y_train1, X_test1, y_test1, \"FS1\")\n",
    "\n",
    "print(\"\\nüìä Performance on Feature Set 1:\")\n",
    "display(results_fs1.sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac05e4",
   "metadata": {},
   "source": [
    "## Feature Set 2 ‚Äî All PSS-10 + All PHQ-9 (19 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d073cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = fs2_train.drop(columns=[\"DepressionEncoded\"])\n",
    "y_train2 = fs2_train[\"DepressionEncoded\"]\n",
    "X_test2  = fs2_test.drop(columns=[\"DepressionEncoded\"])\n",
    "y_test2  = fs2_test[\"DepressionEncoded\"]\n",
    "\n",
    "results_fs2 = train_and_evaluate_all_models(X_train2, y_train2, X_test2, y_test2, \"FS2\")\n",
    "\n",
    "print(\"\\nüìä Performance on Feature Set 2:\")\n",
    "display(results_fs2.sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e715580",
   "metadata": {},
   "source": [
    "## Feature Set 3 ‚Äî All GAD-7 + All PHQ-9 (17 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = fs3_train.drop(columns=[\"DepressionEncoded\"])\n",
    "y_train3 = fs3_train[\"DepressionEncoded\"]\n",
    "X_test3  = fs3_test.drop(columns=[\"DepressionEncoded\"])\n",
    "y_test3  = fs3_test[\"DepressionEncoded\"]\n",
    "\n",
    "results_fs3 = train_and_evaluate_all_models(X_train3, y_train3, X_test3, y_test3, \"FS3\")\n",
    "\n",
    "print(\"\\nüìä Performance on Feature Set 3:\")\n",
    "display(results_fs3.sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b98fd0",
   "metadata": {},
   "source": [
    "## Save Performance Results\n",
    "All model performance tables are saved in `../data/processed/` for documentation and future comparison with Deep Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f90024",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fs1.to_csv(DATA_DIR / \"results_fs1_traditional.csv\", index=False)\n",
    "results_fs2.to_csv(DATA_DIR / \"results_fs2_traditional.csv\", index=False)\n",
    "results_fs3.to_csv(DATA_DIR / \"results_fs3_traditional.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ All results saved to data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
